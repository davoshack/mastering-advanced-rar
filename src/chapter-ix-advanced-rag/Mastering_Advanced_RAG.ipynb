{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using a Query Engine to Answer Queries\n",
    "\n",
    "• Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2008%20-%20Mastering_Advanced_RAG.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "A  [Query Engine](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html)  is an advanced interface that allows interaction with data via natural language queries. It’s a wrapper designed to process queries and generate responses. Combining multiple query engines can enhance functionality, meeting the complexity of specific queries.\n",
    "\n",
    "On the other hand, a  [Chat Engine](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html)  is suitable for an interactive experience like a conversation, as it requires a series of queries and responses. This offers a more dynamic and engaging way to interact with data.\n",
    "\n",
    "To create a query engine, one typically uses the `.as_query_engine()` method on generated indices. Here are the steps included in creating indexes from text files and using query engines to engage with the dataset:\n",
    "\n",
    "Install necessary packages using the command: `!pip install -q llama-index==0.9.14.post3 deeplake==3.8.8 openai==1.3.8 cohere==4.37.`\n",
    "\n",
    "And set up the API key environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from advanced_rag_custom_utils.helper import get_openai_api_key, get_activeloop_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a text file as your source document. We used a file containing a collection of essays by Paul Graham consolidated into a single text file. You can also download this file directly from [towardsai.net/book](http://towardsai.net/book). Alternatively, you can use the following commands in your terminal to create a directory and download the file into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p './paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O './paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `SimpleDirectoryReader` in the LlamaIndex framework to read all files from the designated directory. This class is designed to automatically navigate through the files, converting them into Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the `ServiceContext` to break the lengthy single document into numerous smaller chunks with some overlap. Following that, we will make nodes from the generated documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512, chunk_overlap=64)\n",
    "node_parser = service_context.node_parser\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes should be stored in a vector database for convenient access. The `DeepLakeVectorStore` class can create an empty dataset by specifying a path. You can access the processed dataset using the genai360 organization ID or update it to match your *Activeloop* username and store the data on your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "\n",
    "my_activeloop_org_id = ACTIVELOOP_API_KEY\n",
    "my_activeloop_dataset_name = \"LlamaIndex_paulgraham_essays\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Create an index over the documnts\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-advanced-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
