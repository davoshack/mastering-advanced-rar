{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Metrics & Evaluation\n",
    "\n",
    "‚Ä¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2008%20-%20RAG_Metrics%26Evaluation.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "## RAG (and LLM) Metrics\n",
    "\n",
    "Evaluating LLMs and retrieval-augmented generation (RAG) systems requires a detailed analysis of individual components and the system as a whole. Setting baseline values for pieces, such as chunking logic and embedding models, then assessing each part individually and end-to-end is critical for understanding the impact of changes on the system‚Äôs overall performance. Holistic modules in these evaluations don‚Äôt always require ground-truth labels, as they can be assessed based on the language model‚Äôs query, context, response, and interpretations.\n",
    "\n",
    "Here are five commonly used metrics for evaluating your systems:\n",
    "\n",
    "1.  [Correctness](https://docs.llamaindex.ai/en/latest/examples/evaluation/correctness_eval.html): This metric assesses whether the generated answer aligns with a given query‚Äôs reference answer. It requires labels and involves verifying the accuracy of the generated answer by comparing it to a predefined reference answer.\n",
    "2.  [Faithfulness](https://docs.llamaindex.ai/en/latest/examples/evaluation/faithfulness_eval.html): This evaluates the integrity of the answer concerning the retrieved contexts. The faithfulness metric ensures that the answer accurately reflects the information in the retrieved context, free from distortions or fabrications that might misrepresent the source material.\n",
    "3.  [Context Relevancy](https://docs.llamaindex.ai/en/latest/examples/evaluation/relevancy_eval.html): This measures the relevance of the retrieved context and the resulting answer to the original query. The goal is to ensure the system retrieves relevant information to the user‚Äôs request.\n",
    "4.  [Guideline Adherence](https://docs.llamaindex.ai/en/latest/examples/evaluation/guideline_eval.html): This metric determines if the predicted answer follows established guidelines. It checks whether the response meets predefined criteria, including stylistic, factual, and ethical standards, ensuring the answer responds to the query and adheres to specific norms.\n",
    "5.  [Embedding Semantic Similarity](https://docs.llamaindex.ai/en/latest/examples/evaluation/semantic_similarity_eval.html): This involves calculating a similarity score between the generated and reference answers‚Äô embeddings. It requires reference labels and helps estimate the closeness of the generated response to a reference in terms of semantic content.\n",
    "\n",
    "üí°You can find the documentation pages for the above metrics at [towardsai.net/book.](http://towardsai.net/book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of RAG applications begins with a focus on their primary objective: generate useful outputs supported by contextually relevant facts obtained from retrievers. This analysis then zooms in on specific evaluation metrics such as faithfulness, answer relevancy, and the  [Sensibleness and Specificity Average (SSA)](https://arxiv.org/abs/2001.09977). Google‚Äôs SSA metric, which assesses open-domain chatbot responses, evaluates sensibleness (contextual coherence) and specificity (providing detailed and direct responses). Initially involving human evaluators, this metric ensures that outputs are comprehensive and not excessively vague or general.\n",
    "\n",
    "A high faithfulness score does not necessarily correlate with high relevance. For instance, a response that accurately mirrors the context but does not directly address the query would receive a lower score in answer relevance. This could happen mainly if the response contains incomplete or redundant elements, indicating a gap between the accuracy of the provided context and the direct relevance to the question.\n",
    "\n",
    "This section proceeds by showing how to compute the faithfulness score using the `FaithfulnessEvaluator` class from LlamaIndex. This metric focuses on preventing the issue of ‚Äúhallucination‚Äù and examines responses to determine their alignment with the retrieved context. It assesses whether the response is consistent with the context and the initial query and fits with reference answers or guidelines. The output of this evaluation is a boolean value, indicating whether the response has successfully met the criteria for accuracy and faithfulness.\n",
    "\n",
    "To use the FaithfulnessEvaluator, install the required libraries using Python‚Äôs package manager using !pip install -q llama-index==0.9.14.post3 deeplake==3.8.12 openai==1.3.8 cohere==4.37.\n",
    "\n",
    "Following this, set the API keys for OpenAI and Activeloop and replace the placeholders in the provided code with their respective API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from advanced_rag_custom_utils.helper import get_openai_api_key, get_activeloop_api_key, get_cohere_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()\n",
    "COHERE_API_KEY = get_cohere_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs an example for evaluating a single response for faithfulness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# build service context\n",
    "llm = OpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_store = DeepLakeVectorStore(\n",
    "dataset_path=\"hub://genai360/LlamaIndex_paulgraham_essay\", overwrite=False\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# define evaluator\n",
    "evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "\n",
    "# query index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"What does Paul Graham do?\"\n",
    ")\n",
    "\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "\n",
    "print(\"> response:\", response)\n",
    "print(\"> evaluator result:\", eval_result.passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> response: Paul Graham is involved in various activities. He is a writer and has given talks on topics such as starting a startup. He has also worked on software development, including creating software for generating websites and building online stores. Additionally, he has been a studio assistant for a beloved teacher who is a painter.  \n",
    "> evaluator result: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the above code was previously discussed and will likely be familiar. It includes creating an index from the Deep Lake vector database, using it to make queries to the LLM, and performing the evaluation procedure. In this process, the query engine answers the question and sends its response to the evaluator for further examination.\n",
    "\n",
    "In this case, the code creates a `FaithfulnessEvaluator` object, a mechanism for evaluating the precision of responses produced by the language model, GPT-4 Turbo. This evaluator operates using the previously defined `service_context`, which contains the configured GPT-4 Turbo model and provides the settings and parameters required for the language model‚Äôs optimal functioning.\n",
    "\n",
    "The fundamental responsibility of the `FaithfulnessEvaluator` is assessing the extent to which the responses from the language model align with accurate and reliable information. It uses a series of criteria or algorithms to accomplish this, comparing the model-generated responses with verified factual data or anticipated results.\n",
    "\n",
    "The evaluator proceeds to examine the response for its commitment to factual accuracy. This involves determining if the response accurately and dependably represents historical facts related to the query. The outcome of this assessment (`eval_result`) is then reviewed to ascertain if it aligns with the accuracy benchmarks established by the evaluator, as denoted by `eval_result.passing`. The result returns a Boolean (here `True`) value indicating whether the response passed the accuracy and faithfulness checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Specific Evaluation Metrics\n",
    "\n",
    "The evaluation of retrieval in RAG systems (not just LLM generation quality) involves determining the relevance of documents to specific queries. In information retrieval, the main goal is to identify unstructured data that meets a particular information requirement within a database.\n",
    "\n",
    ">‚ú¥Ô∏è RAG system outputs need three critical aspects: factual accuracy, direct relevance to the query, and inclusion of essential contextual information.\n",
    "\n",
    "![image](evaluation-metrics-of-retrieval-in-rag.jpg)\n",
    "##### *The evaluation metrics of retrieval in RAG systems*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics used to assess the effectiveness of a retrieval system mostly include Mean Reciprocal Rank (MRR), Hit Rate, Mean Average Precision (MAP), and Normalized Discounted Cumulative Gain (NDCG):\n",
    "\n",
    "-   **Mean Reciprocal Rank (MRR)**  measures the effectiveness of a system by averaging the reciprocal of the rank of the first relevant item in the search results. Higher MRR indicates better performance, as relevant items appear earlier.\n",
    "-   **Hit Rate**  evaluates how often a relevant item appears within the top N results. It is a simple accuracy measure indicating the presence of at least one relevant item in the retrieved set.\n",
    "-   **Mean Average Precision (MAP)**  calculates the average precision at each relevant item across multiple queries. It provides a single measure of quality by considering the precision of relevant items‚Äô placements within the ranked results.\n",
    "-   **Normalized Discounted Cumulative Gain (NDCG)**  evaluates the quality of the ranking by considering both the position and relevance of items. It discounts the relevance of items lower in the ranking, rewarding systems that rank highly relevant items earlier.\n",
    "\n",
    "The `RetrieverEvaluator` from LlamaIndex is an advanced method to calculate metrics such as Mean Reciprocal Rank (MRR) and Hit Rate. Its primary function is to evaluate the effectiveness of a retrieval system that sources data relevant to user queries from a database or index. This class measures the retriever‚Äôs performance in responding to specific questions and providing the expected results, setting benchmarks for assessment.\n",
    "\n",
    "For this evaluator, it is necessary to compile an evaluation dataset, which includes the content, a set of queries, and corresponding reference points for answering these queries. The `generate_question_context_pairs` function in LlamaIndex can create the evaluation dataset automatically. The process involves inputting a query and using the dataset as a benchmark to ensure the retrieval system accurately sources the correct documents. A detailed [tutorial on using the](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html) RetrieverEvaluator from the LlamaIndex documentation is accessible at [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    ">üí°Although the evaluation of single queries is discussed, real-world applications typically need batch evaluations. This involves extracting a broad range of queries and their anticipated outcomes from the retriever to assess its general reliability. The retriever is evaluated using multiple queries and the expected results during batch testing. The process involves methodically inputting various queries into the retriever and comparing its responses to established correct answers to measure its consistency accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are open-source datasets that can be freely used to evaluate a RAG pipeline. An evaluation dataset comprises a selection of queries carefully paired with the most appropriate sources containing their answers. Such a dataset includes ideal expected answers from the large language model. Each query is paired with the most relevant source from the document, ensuring these sources directly address the queries.\n",
    "\n",
    "Developing an evaluation dataset requires collecting a variety of realistic customer queries and matching them with expert answers. This dataset can be used to evaluate the responses of a language model, ensuring the LLM‚Äôs answers are accurate and relevant compared to expert responses. More information on  [creating this dataset](https://github.com/microsoft/promptflow-resource-hub/blob/main/sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md)  is accessible at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "Once the evaluation dataset is prepared, it can be utilized to assess the quality of responses from the LLM. Following each evaluation, metrics will be available to quantify the user experience, providing valuable insights into the performance of the LLM. For example:\n",
    "\n",
    "![image](assess-quality.jpg)\n",
    "\n",
    ">‚ö†Ô∏è Although generating questions synthetically has its advantages, it is advisable to use questions that are tied to authentic user experiences. The questions employed in these evaluations should closely resemble real user queries, which are usually difficult to accomplish with the large language model. It is recommended to manually create questions emphasizing the users‚Äô viewpoint for a more precise representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-advanced-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
