{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Metrics & Evaluation\n",
    "\n",
    "‚Ä¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2008%20-%20RAG_Metrics%26Evaluation.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "## RAG (and LLM) Metrics\n",
    "\n",
    "Evaluating LLMs and retrieval-augmented generation (RAG) systems requires a detailed analysis of individual components and the system as a whole. Setting baseline values for pieces, such as chunking logic and embedding models, then assessing each part individually and end-to-end is critical for understanding the impact of changes on the system‚Äôs overall performance. Holistic modules in these evaluations don‚Äôt always require ground-truth labels, as they can be assessed based on the language model‚Äôs query, context, response, and interpretations.\n",
    "\n",
    "Here are five commonly used metrics for evaluating your systems:\n",
    "\n",
    "1.  [Correctness](https://docs.llamaindex.ai/en/latest/examples/evaluation/correctness_eval.html): This metric assesses whether the generated answer aligns with a given query‚Äôs reference answer. It requires labels and involves verifying the accuracy of the generated answer by comparing it to a predefined reference answer.\n",
    "2.  [Faithfulness](https://docs.llamaindex.ai/en/latest/examples/evaluation/faithfulness_eval.html): This evaluates the integrity of the answer concerning the retrieved contexts. The faithfulness metric ensures that the answer accurately reflects the information in the retrieved context, free from distortions or fabrications that might misrepresent the source material.\n",
    "3.  [Context Relevancy](https://docs.llamaindex.ai/en/latest/examples/evaluation/relevancy_eval.html): This measures the relevance of the retrieved context and the resulting answer to the original query. The goal is to ensure the system retrieves relevant information to the user‚Äôs request.\n",
    "4.  [Guideline Adherence](https://docs.llamaindex.ai/en/latest/examples/evaluation/guideline_eval.html): This metric determines if the predicted answer follows established guidelines. It checks whether the response meets predefined criteria, including stylistic, factual, and ethical standards, ensuring the answer responds to the query and adheres to specific norms.\n",
    "5.  [Embedding Semantic Similarity](https://docs.llamaindex.ai/en/latest/examples/evaluation/semantic_similarity_eval.html): This involves calculating a similarity score between the generated and reference answers‚Äô embeddings. It requires reference labels and helps estimate the closeness of the generated response to a reference in terms of semantic content.\n",
    "\n",
    "üí°You can find the documentation pages for the above metrics at [towardsai.net/book.](http://towardsai.net/book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of RAG applications begins with a focus on their primary objective: generate useful outputs supported by contextually relevant facts obtained from retrievers. This analysis then zooms in on specific evaluation metrics such as faithfulness, answer relevancy, and the  [Sensibleness and Specificity Average (SSA)](https://arxiv.org/abs/2001.09977). Google‚Äôs SSA metric, which assesses open-domain chatbot responses, evaluates sensibleness (contextual coherence) and specificity (providing detailed and direct responses). Initially involving human evaluators, this metric ensures that outputs are comprehensive and not excessively vague or general.\n",
    "\n",
    "A high faithfulness score does not necessarily correlate with high relevance. For instance, a response that accurately mirrors the context but does not directly address the query would receive a lower score in answer relevance. This could happen mainly if the response contains incomplete or redundant elements, indicating a gap between the accuracy of the provided context and the direct relevance to the question.\n",
    "\n",
    "This section proceeds by showing how to compute the faithfulness score using the `FaithfulnessEvaluator` class from LlamaIndex. This metric focuses on preventing the issue of ‚Äúhallucination‚Äù and examines responses to determine their alignment with the retrieved context. It assesses whether the response is consistent with the context and the initial query and fits with reference answers or guidelines. The output of this evaluation is a boolean value, indicating whether the response has successfully met the criteria for accuracy and faithfulness.\n",
    "\n",
    "To use the FaithfulnessEvaluator, install the required libraries using Python‚Äôs package manager using !pip install -q llama-index==0.9.14.post3 deeplake==3.8.12 openai==1.3.8 cohere==4.37.\n",
    "\n",
    "Following this, set the API keys for OpenAI and Activeloop and replace the placeholders in the provided code with their respective API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from advanced_rag_custom_utils.helper import get_openai_api_key, get_activeloop_api_key, get_cohere_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()\n",
    "COHERE_API_KEY = get_cohere_api_key()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-advanced-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
