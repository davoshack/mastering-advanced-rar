{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Metrics & Evaluation\n",
    "\n",
    "‚Ä¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2008%20-%20RAG_Metrics%26Evaluation.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "## RAG (and LLM) Metrics\n",
    "\n",
    "Evaluating LLMs and retrieval-augmented generation (RAG) systems requires a detailed analysis of individual components and the system as a whole. Setting baseline values for pieces, such as chunking logic and embedding models, then assessing each part individually and end-to-end is critical for understanding the impact of changes on the system‚Äôs overall performance. Holistic modules in these evaluations don‚Äôt always require ground-truth labels, as they can be assessed based on the language model‚Äôs query, context, response, and interpretations.\n",
    "\n",
    "Here are five commonly used metrics for evaluating your systems:\n",
    "\n",
    "1.  [Correctness](https://docs.llamaindex.ai/en/latest/examples/evaluation/correctness_eval.html): This metric assesses whether the generated answer aligns with a given query‚Äôs reference answer. It requires labels and involves verifying the accuracy of the generated answer by comparing it to a predefined reference answer.\n",
    "2.  [Faithfulness](https://docs.llamaindex.ai/en/latest/examples/evaluation/faithfulness_eval.html): This evaluates the integrity of the answer concerning the retrieved contexts. The faithfulness metric ensures that the answer accurately reflects the information in the retrieved context, free from distortions or fabrications that might misrepresent the source material.\n",
    "3.  [Context Relevancy](https://docs.llamaindex.ai/en/latest/examples/evaluation/relevancy_eval.html): This measures the relevance of the retrieved context and the resulting answer to the original query. The goal is to ensure the system retrieves relevant information to the user‚Äôs request.\n",
    "4.  [Guideline Adherence](https://docs.llamaindex.ai/en/latest/examples/evaluation/guideline_eval.html): This metric determines if the predicted answer follows established guidelines. It checks whether the response meets predefined criteria, including stylistic, factual, and ethical standards, ensuring the answer responds to the query and adheres to specific norms.\n",
    "5.  [Embedding Semantic Similarity](https://docs.llamaindex.ai/en/latest/examples/evaluation/semantic_similarity_eval.html): This involves calculating a similarity score between the generated and reference answers‚Äô embeddings. It requires reference labels and helps estimate the closeness of the generated response to a reference in terms of semantic content.\n",
    "\n",
    "üí°You can find the documentation pages for the above metrics at [towardsai.net/book.](http://towardsai.net/book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of RAG applications begins with a focus on their primary objective: generate useful outputs supported by contextually relevant facts obtained from retrievers. This analysis then zooms in on specific evaluation metrics such as faithfulness, answer relevancy, and the  [Sensibleness and Specificity Average (SSA)](https://arxiv.org/abs/2001.09977). Google‚Äôs SSA metric, which assesses open-domain chatbot responses, evaluates sensibleness (contextual coherence) and specificity (providing detailed and direct responses). Initially involving human evaluators, this metric ensures that outputs are comprehensive and not excessively vague or general.\n",
    "\n",
    "A high faithfulness score does not necessarily correlate with high relevance. For instance, a response that accurately mirrors the context but does not directly address the query would receive a lower score in answer relevance. This could happen mainly if the response contains incomplete or redundant elements, indicating a gap between the accuracy of the provided context and the direct relevance to the question.\n",
    "\n",
    "This section proceeds by showing how to compute the faithfulness score using the `FaithfulnessEvaluator` class from LlamaIndex. This metric focuses on preventing the issue of ‚Äúhallucination‚Äù and examines responses to determine their alignment with the retrieved context. It assesses whether the response is consistent with the context and the initial query and fits with reference answers or guidelines. The output of this evaluation is a boolean value, indicating whether the response has successfully met the criteria for accuracy and faithfulness.\n",
    "\n",
    "To use the FaithfulnessEvaluator, install the required libraries using Python‚Äôs package manager using !pip install -q llama-index==0.9.14.post3 deeplake==3.8.12 openai==1.3.8 cohere==4.37.\n",
    "\n",
    "Following this, set the API keys for OpenAI and Activeloop and replace the placeholders in the provided code with their respective API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from advanced_rag_custom_utils.helper import get_openai_api_key, get_activeloop_api_key, get_cohere_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()\n",
    "COHERE_API_KEY = get_cohere_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs an example for evaluating a single response for faithfulness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# build service context\n",
    "llm = OpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_store = DeepLakeVectorStore(\n",
    "dataset_path=\"hub://genai360/LlamaIndex_paulgraham_essay\", overwrite=False\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# define evaluator\n",
    "evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "\n",
    "# query index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"What does Paul Graham do?\"\n",
    ")\n",
    "\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "\n",
    "print(\"> response:\", response)\n",
    "print(\"> evaluator result:\", eval_result.passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> response: Paul Graham is involved in various activities. He is a writer and has given talks on topics such as starting a startup. He has also worked on software development, including creating software for generating websites and building online stores. Additionally, he has been a studio assistant for a beloved teacher who is a painter.  \n",
    "> evaluator result: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the above code was previously discussed and will likely be familiar. It includes creating an index from the Deep Lake vector database, using it to make queries to the LLM, and performing the evaluation procedure. In this process, the query engine answers the question and sends its response to the evaluator for further examination.\n",
    "\n",
    "In this case, the code creates a `FaithfulnessEvaluator` object, a mechanism for evaluating the precision of responses produced by the language model, GPT-4 Turbo. This evaluator operates using the previously defined `service_context`, which contains the configured GPT-4 Turbo model and provides the settings and parameters required for the language model‚Äôs optimal functioning.\n",
    "\n",
    "The fundamental responsibility of the `FaithfulnessEvaluator` is assessing the extent to which the responses from the language model align with accurate and reliable information. It uses a series of criteria or algorithms to accomplish this, comparing the model-generated responses with verified factual data or anticipated results.\n",
    "\n",
    "The evaluator proceeds to examine the response for its commitment to factual accuracy. This involves determining if the response accurately and dependably represents historical facts related to the query. The outcome of this assessment (`eval_result`) is then reviewed to ascertain if it aligns with the accuracy benchmarks established by the evaluator, as denoted by `eval_result.passing`. The result returns a Boolean (here `True`) value indicating whether the response passed the accuracy and faithfulness checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Specific Evaluation Metrics\n",
    "\n",
    "The evaluation of retrieval in RAG systems (not just LLM generation quality) involves determining the relevance of documents to specific queries. In information retrieval, the main goal is to identify unstructured data that meets a particular information requirement within a database.\n",
    "\n",
    ">‚ú¥Ô∏è RAG system outputs need three critical aspects: factual accuracy, direct relevance to the query, and inclusion of essential contextual information.\n",
    "\n",
    "![image](evaluation-metrics-of-retrieval-in-rag.jpg)\n",
    "##### *The evaluation metrics of retrieval in RAG systems*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics used to assess the effectiveness of a retrieval system mostly include Mean Reciprocal Rank (MRR), Hit Rate, Mean Average Precision (MAP), and Normalized Discounted Cumulative Gain (NDCG):\n",
    "\n",
    "-   **Mean Reciprocal Rank (MRR)**  measures the effectiveness of a system by averaging the reciprocal of the rank of the first relevant item in the search results. Higher MRR indicates better performance, as relevant items appear earlier.\n",
    "-   **Hit Rate**  evaluates how often a relevant item appears within the top N results. It is a simple accuracy measure indicating the presence of at least one relevant item in the retrieved set.\n",
    "-   **Mean Average Precision (MAP)**  calculates the average precision at each relevant item across multiple queries. It provides a single measure of quality by considering the precision of relevant items‚Äô placements within the ranked results.\n",
    "-   **Normalized Discounted Cumulative Gain (NDCG)**  evaluates the quality of the ranking by considering both the position and relevance of items. It discounts the relevance of items lower in the ranking, rewarding systems that rank highly relevant items earlier.\n",
    "\n",
    "The `RetrieverEvaluator` from LlamaIndex is an advanced method to calculate metrics such as Mean Reciprocal Rank (MRR) and Hit Rate. Its primary function is to evaluate the effectiveness of a retrieval system that sources data relevant to user queries from a database or index. This class measures the retriever‚Äôs performance in responding to specific questions and providing the expected results, setting benchmarks for assessment.\n",
    "\n",
    "For this evaluator, it is necessary to compile an evaluation dataset, which includes the content, a set of queries, and corresponding reference points for answering these queries. The `generate_question_context_pairs` function in LlamaIndex can create the evaluation dataset automatically. The process involves inputting a query and using the dataset as a benchmark to ensure the retrieval system accurately sources the correct documents. A detailed [tutorial on using the](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html) RetrieverEvaluator from the LlamaIndex documentation is accessible at [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    ">üí°Although the evaluation of single queries is discussed, real-world applications typically need batch evaluations. This involves extracting a broad range of queries and their anticipated outcomes from the retriever to assess its general reliability. The retriever is evaluated using multiple queries and the expected results during batch testing. The process involves methodically inputting various queries into the retriever and comparing its responses to established correct answers to measure its consistency accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are open-source datasets that can be freely used to evaluate a RAG pipeline. An evaluation dataset comprises a selection of queries carefully paired with the most appropriate sources containing their answers. Such a dataset includes ideal expected answers from the large language model. Each query is paired with the most relevant source from the document, ensuring these sources directly address the queries.\n",
    "\n",
    "Developing an evaluation dataset requires collecting a variety of realistic customer queries and matching them with expert answers. This dataset can be used to evaluate the responses of a language model, ensuring the LLM‚Äôs answers are accurate and relevant compared to expert responses. More information on  [creating this dataset](https://github.com/microsoft/promptflow-resource-hub/blob/main/sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md)  is accessible at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "Once the evaluation dataset is prepared, it can be utilized to assess the quality of responses from the LLM. Following each evaluation, metrics will be available to quantify the user experience, providing valuable insights into the performance of the LLM. For example:\n",
    "\n",
    "![image](assess-quality.jpg)\n",
    "\n",
    ">‚ö†Ô∏è Although generating questions synthetically has its advantages, it is advisable to use questions that are tied to authentic user experiences. The questions employed in these evaluations should closely resemble real user queries, which are usually difficult to accomplish with the large language model. It is recommended to manually create questions emphasizing the users‚Äô viewpoint for a more precise representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community-Based Evaluation Tools\n",
    "\n",
    "LlamaIndex includes various evaluation tools designed to encourage community engagement and collaborative efforts. These tools facilitate a collective process of assessing and improving the system. LlamaIndex enables the easy integration of constant feedback, allowing users and developers to participate actively in the evaluation phase. Notable tools in this ecosystem include  [Ragas](https://github.com/explodinggradients/ragas/blob/main/docs/howtos/integrations/llamaindex.ipynb), an important tool with extensive metrics for assessing and integrating with LlamaIndex, and DeepEval, a tool for in-depth review and complete assessments of several areas of the system.\n",
    "\n",
    "### Evaluating with Ragas\n",
    "\n",
    "Ragas‚Äôs evaluation process revolves around utilizing specific metrics, including faithfulness, answer relevancy, context precision, context recall, and harmfulness. This process has several key elements. The performance of the query engine is the central focus of the assessment, as it plays a crucial role in the evaluation. Ragas provides a range of metrics specifically designed to facilitate a comprehensive analysis of the engine‚Äôs capabilities. Finally, a carefully curated set of questions is used to test the engine‚Äôs proficiency in retrieving and generating accurate responses.\n",
    "\n",
    "The first step to using the Ragas library is setting up a query engine. This process involves loading a document. We will use the ‚ÄúNew York City‚Äù Wikipedia page as the source document in this case. Next, you will need to install two more libraries: one to process the webpage content, and the other is a prerequisite for the evaluation library: `!pip install html2text==2020.1.16 ragas==0.0.22`.\n",
    "\n",
    "Load the content by passing a URL to the `SimpleWebPageReader` class. Use these documents to build the index and query engine. Now, you can try asking questions about the document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "[\"https://en.wikipedia.org/wiki/New_York_City\"]\n",
    ")\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=ServiceContext.from_defaults(chunk_size=512)\n",
    ")\n",
    "\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector = query_engine.query(\"How did New York City get its name?\")\n",
    "\n",
    "print(response_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    New York City got its name in honor of the Duke of York, who later became King    James II of England. The Duke of York was appointed as the proprietor of the former territory of New Netherland, including the city of New Amsterdam, when England seized it from Dutch control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step involves composing questions, ideally derived from the original documents, which can be done using an LLM like GPT-4. Usually, one would then use those questions to get their answers through the RAG pipeline and evaluate them against the true answers. For the purpose of this example, we write the true answers by hand (i.e., the `eval_answers` list) and include both correct and incorrect answers so that we won‚Äôt get perfect scores in the computed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What is the population of New York City as of 2020?\",\n",
    "    \"Which borough of New York City has the highest population?\",\n",
    "    \"What is the economic significance of New York City?\",\n",
    "    \"How did New York City get its name?\",\n",
    "    \"What is the significance of the Statue of Liberty in New York City?\",\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"8,804,000\",  # incorrect answer\n",
    "    \"Queens\",  # incorrect answer\n",
    "    \"\"\"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\"\"\",\n",
    "    \"\"\"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\"\"\",\n",
    "    \"\"\"The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\"\"\",\n",
    "]\n",
    "\n",
    "eval_answers = [[a] for a in eval_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the setup stage of the evaluation process. The competency of the `QueryEngine` is evaluated based on how well it processes and replies to these specific questions, with the responses serving as a baseline for measuring performance. The metrics from the Ragas library must be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    harmfulness,\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics list groups the metrics into a collection, which can be used in the assessment process to evaluate various elements of the QueryEngine‚Äôs performance. The results will include scores for each metric. Finally, let‚Äôs run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llama_index import evaluate\n",
    "\n",
    "result = evaluate(query_engine, metrics, eval_questions, eval_answers)\n",
    "\n",
    "# print the final scores\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    evaluating with [faithfulness]  \n",
    "    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:16<00:00, 16.95s/it]  \n",
    "    evaluating with [answer_relevancy]  \n",
    "    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00, 3.54s/it]  \n",
    "    evaluating with [context_precision]  \n",
    "    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00, 2.73s/it]  \n",
    "    evaluating with [context_recall]  \n",
    "    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00, 7.06s/it]  \n",
    "    evaluating with [harmfulness]  \n",
    "    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00, 2.16s/it]  \n",
    "      \n",
    "    {'faithfulness': 0.8000, 'answer_relevancy': 0.7634, 'context_precision': 0.6000,  \n",
    "    'context_recall': 0.8667, 'harmfulness': 0.0000}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system‚Äôs responses were evaluated across several metrics, each providing insight into different aspects of its performance.\n",
    "\n",
    "First, the faithfulness score was 0.8, indicating a relatively high level of accuracy. This means the responses largely adhered to the factual content of the source material. Next, the system achieved an answer relevancy score of 0.7634. This high score suggests that most of the responses closely aligned with the intent of the given queries, ensuring that the answers were pertinent to the questions asked.\n",
    "\n",
    "The context precision score, on the other hand, was 0.6. This indicates that while the system sometimes included relevant information, it also occasionally incorporated irrelevant context. In contrast, the context recall score was quite high, at 0.8667. This suggests that the system effectively identified and utilized the most relevant contexts in generating its final answers.\n",
    "\n",
    "Lastly, the harmfulness score was 0, indicating that none of the responses contained harmful or inappropriate content. This metric reflects the system‚Äôs success in maintaining safe and suitable interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-advanced-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
