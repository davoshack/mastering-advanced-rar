{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain LangSmith and LangChain Hub\n",
    "\n",
    "-   Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2008%20-%20LangSmith_Introduction.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "üí°While LangChain is appropriate for initial prototyping, LangSmith provides a setting for debugging, testing, and refining LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangSmith](https://www.langchain.com/langsmith)  is a platform for evaluating and monitoring the quality of LLM systems‚Äô outputs. Its functionality includes tracking metadata, token usage, and execution time, which is vital for managing resources effectively.\n",
    "\n",
    "LangSmith improves the efficiency and performance of new chains and tools. It also provides visualization tools to recognize response patterns and trends, enhancing the understanding and analysis of performance. It allows users to create customized testing environments tailored to specific requirements, enabling comprehensive evaluation under various conditions. The platform also tracks the executions linked to an active instance and allows for the testing and assessing of any prompts or responses produced. LangSmith offers several tutorials and in-depth documentation to assist users in getting started.\n",
    "\n",
    "On the other hand, LangChain Hub is a platform designed for the development, storage, and sharing of reusable components used in building applications with large language models (LLMs). It offers a repository of modules such as prompts, chains, and agents that can be easily accessed and integrated into projects. LangChain Hub simplifies the creation of LLM-driven applications by providing tools and templates and promoting collaboration among developers.\n",
    "\n",
    "This section contains an example of how to use LangChain Hub together with a question-answering chain in LangChain. We will guide you through the setup process for LangChain, including installing necessary libraries and configuring environment variables. A LangSmith account is required for certain features like tracing. Detailed steps for setting up a new account will be provided.\n",
    "\n",
    "First, you will need an API key. Navigate to the  [LangSmith](https://www.langchain.com/langsmith)  website and register for an account. Find the option to generate an API key on the settings page. Click the ‚ÄúGenerate API Key‚Äù button to receive your API key.\n",
    "\n",
    "Install the necessary libraries using the command `!pip install -q langchain==0.0.346 openai==1.3.7 tiktoken==0.5.2 cohere==4.37 deeplake==3.8.11 langchainhub==0.1.14`.\n",
    "\n",
    "Configure the environment with the API keys for OpenAI, which will be used in the embedding generation process, and the Activeloop key, which will be used to store data in the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advanced_rag_custom_utils.helper import (\n",
    "    get_openai_api_key, \n",
    "    get_activeloop_api_key, \n",
    "    get_langchain_api_key,\n",
    "    get_langchain_project,\n",
    "    get_langchain_tracing_v2,\n",
    "    get_langchain_endpoint,\n",
    ")\n",
    "\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()\n",
    "\n",
    "LANGCHAIN_API_KEY = get_langchain_api_key()\n",
    "LANGCHAIN_TRACING_V2 = get_langchain_tracing_v2()\n",
    "LANGCHAIN_PROJECT = get_langchain_project()\n",
    "LANGCHAIN_ENDPOINT = get_langchain_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how to commit a prompt to the LangChain Hub by adding it to your handle‚Äôs namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\")\n",
    "\n",
    "handle = \"<YOUR_USERNAME>\"\n",
    "hub.push(f\"{handle}/rag-prompt\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you update the prompt, you can push the modified prompt to the same key to ‚Äúcommit‚Äù a new version of the prompt during evaluation. Let‚Äôs say we want to add a system message to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may try making other changes and saving them in a new commit.\n",
    "from langchain import schema\n",
    "\n",
    "prompt.messages.insert(0,\n",
    "   schema.SystemMessage(\n",
    "       content=\"You are a precise, autoregressive question-answering system.\"\n",
    "   )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent version of the prompt is kept as the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing to the same prompt \"repo\" will create a new commit\n",
    "hub.push(f\"{handle}/rag-prompt\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith also offers a feature for reviewing the inputs and outputs of each component within a chain, making it easier to log runs for large language model applications. This functionality is particularly beneficial when debugging your application or understanding the behavior of certain components. For more information on  [Tracing](http://towardsai.net/book), visit the LangChain documentation.\n",
    "\n",
    "The next steps involve loading data from a webpage, dividing it into smaller chunks, converting these segments into embeddings, and then storing them in the Deep Lake vector database. This process also includes prompt templates from the  [LangSmith Hub](https://smith.langchain.com/hub).\n",
    "\n",
    "The content of a webpage can be read using the `WebBaseLoader` class. This class will provide a single instance of the `Document` class with all the text from the URL. Subsequently, this large text is divided into smaller chunks, each comprising 500 characters without overlapping, resulting in 130 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Loading\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "print(len(data))\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(len(all_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1  \n",
    "    130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunks can be saved in the Deep Lake vector database using the LangChain integration. The `DeepLake` class transforms texts into embeddings using `OpenAI‚Äôs` API and stores these results in the cloud. You can use your own organization name (your username by default) to create the dataset. Note that this task involves the costs of utilizing OpenAI endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "vectorstore = DeepLake.from_documents(\n",
    "                        all_splits,\n",
    "                        dataset_path=\"hub://genai360/langsmith_intro\",\n",
    "                        embedding=OpenAIEmbeddings(), overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Your Deep Lake dataset has been successfully created!  \n",
    "    Creating 130 embeddings in 1 batches of size 130:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00, 5.81s/it] dataset (path='hub://genai360/langsmith_intro', tensors=['text', 'metadata', 'embedding', 'id'])  \n",
    "      \n",
    "    tensor htype shape dtype compression  \n",
    "    ------- ------- ------- ------- -------  \n",
    "    text text (130, 1) str None  \n",
    "    metadata json (130, 1) str None  \n",
    "    embedding embedding (130, 1536) float32 None  \n",
    "    id text (130, 1) str None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing the data, select a prompt from the LangChain Hub, getting a `ChatPromptTemplate` instance. Using a Prompt Template removes trial and error and allows using already-tested implementations. The code below tags a specific version of the prompt to ensure that future changes do not affect the version currently deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt:50442af1\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the `RetrievalQA` chain to retrieve related documents from the database and then the `ChatOpenAI` model to build the final response using these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The approaches to task decomposition include using LLM with simple prompting, task-specific instructions, and human inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt versioning encourages continual experimentation and collaboration, preventing the unintentional deployment of chain components without thorough testing. Leveraging the LangChain Hub and prompt versioning provides an easy and organized approach to many LLM-related tasks. It ensures reliable performance by using pre-tested, versioned prompts, reducing the need for trial and error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-advanced-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
